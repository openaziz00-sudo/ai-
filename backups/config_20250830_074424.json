{
  "timestamp": "20250830_074424",
  "files": {
    "requirements.txt": {
      "content": "# dr.x Workflows API Dependencies\n# Core API Framework\nfastapi>=0.104.0,<1.0.0\nuvicorn[standard]>=0.24.0,<1.0.0\npydantic>=2.4.0,<3.0.0\nslowapi\nlimits",
      "size": 149,
      "modified": "2025-08-30T04:42:56"
    },
    "api_server.py": {
      "content": "#!/usr/bin/env python3\n\"\"\"\nFastAPI Server for dr.x Workflow Documentation\nHigh-performance API with sub-100ms response times.\n\"\"\"\n\nfrom fastapi import FastAPI, HTTPException, Query, BackgroundTasks\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import HTMLResponse, FileResponse, JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.middleware.gzip import GZipMiddleware\nfrom pydantic import BaseModel, field_validator\nfrom typing import Optional, List, Dict, Any\nimport json\nimport os\nimport asyncio\nfrom pathlib import Path\nimport uvicorn\n\nfrom workflow_db import WorkflowDatabase\n\n# Initialize FastAPI app\napp = FastAPI(\n    title=\"dr.x Workflow Documentation API\",\n    description=\"Fast API for browsing and searching workflow documentation\",\n    version=\"2.0.0\"\n)\n\n# Add middleware for performance\napp.add_middleware(GZipMiddleware, minimum_size=1000)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Initialize database\ndb = WorkflowDatabase()\n\n# Startup function to verify database\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Verify database connectivity on startup.\"\"\"\n    try:\n        stats = db.get_stats()\n        if stats['total'] == 0:\n            print(\"\u26a0\ufe0f  Warning: No workflows found in database. Run indexing first.\")\n        else:\n            print(f\"\u2705 Database connected: {stats['total']} workflows indexed\")\n    except Exception as e:\n        print(f\"\u274c Database connection failed: {e}\")\n        raise\n\n# Response models\nclass WorkflowSummary(BaseModel):\n    id: Optional[int] = None\n    filename: str\n    name: str\n    active: bool\n    description: str = \"\"\n    trigger_type: str = \"Manual\"\n    complexity: str = \"low\"\n    node_count: int = 0\n    integrations: List[str] = []\n    tags: List[str] = []\n    created_at: Optional[str] = None\n    updated_at: Optional[str] = None\n    \n    class Config:\n        # Allow conversion of int to bool for active field\n        validate_assignment = True\n        \n    @field_validator('active', mode='before')\n    @classmethod\n    def convert_active(cls, v):\n        if isinstance(v, int):\n            return bool(v)\n        return v\n    \n\nclass SearchResponse(BaseModel):\n    workflows: List[WorkflowSummary]\n    total: int\n    page: int\n    per_page: int\n    pages: int\n    query: str\n    filters: Dict[str, Any]\n\nclass StatsResponse(BaseModel):\n    total: int\n    active: int\n    inactive: int\n    triggers: Dict[str, int]\n    complexity: Dict[str, int]\n    total_nodes: int\n    unique_integrations: int\n    last_indexed: str\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Serve the main documentation page.\"\"\"\n    static_dir = Path(\"static\")\n    index_file = static_dir / \"index.html\"\n    if not index_file.exists():\n        return HTMLResponse(\"\"\"\n        <html><body>\n        <h1>Setup Required</h1>\n        <p>Static files not found. Please ensure the static directory exists with index.html</p>\n        <p>Current directory: \"\"\" + str(Path.cwd()) + \"\"\"</p>\n        </body></html>\n        \"\"\")\n    return FileResponse(str(index_file))\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\"status\": \"healthy\", \"message\": \"dr.x Workflow API is running\"}\n\n@app.get(\"/api/stats\", response_model=StatsResponse)\nasync def get_stats():\n    \"\"\"Get workflow database statistics.\"\"\"\n    try:\n        stats = db.get_stats()\n        return StatsResponse(**stats)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error fetching stats: {str(e)}\")\n\n@app.get(\"/api/workflows\", response_model=SearchResponse)\nasync def search_workflows(\n    q: str = Query(\"\", description=\"Search query\"),\n    trigger: str = Query(\"all\", description=\"Filter by trigger type\"),\n    complexity: str = Query(\"all\", description=\"Filter by complexity\"),\n    active_only: bool = Query(False, description=\"Show only active workflows\"),\n    page: int = Query(1, ge=1, description=\"Page number\"),\n    per_page: int = Query(20, ge=1, le=100, description=\"Items per page\")\n):\n    \"\"\"Search and filter workflows with pagination.\"\"\"\n    try:\n        offset = (page - 1) * per_page\n        \n        workflows, total = db.search_workflows(\n            query=q,\n            trigger_filter=trigger,\n            complexity_filter=complexity,\n            active_only=active_only,\n            limit=per_page,\n            offset=offset\n        )\n        \n        # Convert to Pydantic models with error handling\n        workflow_summaries = []\n        for workflow in workflows:\n            try:\n                # Remove extra fields that aren't in the model\n                clean_workflow = {\n                    'id': workflow.get('id'),\n                    'filename': workflow.get('filename', ''),\n                    'name': workflow.get('name', ''),\n                    'active': workflow.get('active', False),\n                    'description': workflow.get('description', ''),\n                    'trigger_type': workflow.get('trigger_type', 'Manual'),\n                    'complexity': workflow.get('complexity', 'low'),\n                    'node_count': workflow.get('node_count', 0),\n                    'integrations': workflow.get('integrations', []),\n                    'tags': workflow.get('tags', []),\n                    'created_at': workflow.get('created_at'),\n                    'updated_at': workflow.get('updated_at')\n                }\n                workflow_summaries.append(WorkflowSummary(**clean_workflow))\n            except Exception as e:\n                print(f\"Error converting workflow {workflow.get('filename', 'unknown')}: {e}\")\n                # Continue with other workflows instead of failing completely\n                continue\n        \n        pages = (total + per_page - 1) // per_page  # Ceiling division\n        \n        return SearchResponse(\n            workflows=workflow_summaries,\n            total=total,\n            page=page,\n            per_page=per_page,\n            pages=pages,\n            query=q,\n            filters={\n                \"trigger\": trigger,\n                \"complexity\": complexity,\n                \"active_only\": active_only\n            }\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error searching workflows: {str(e)}\")\n\n@app.get(\"/api/workflows/{filename}\")\nasync def get_workflow_detail(filename: str):\n    \"\"\"Get detailed workflow information including raw JSON.\"\"\"\n    try:\n        # Get workflow metadata from database\n        workflows, _ = db.search_workflows(f'filename:\"{filename}\"', limit=1)\n        if not workflows:\n            raise HTTPException(status_code=404, detail=\"Workflow not found in database\")\n        \n        workflow_meta = workflows[0]\n        \n        # Load raw JSON from file\n        file_path = os.path.join(\"workflows\", filename)\n        if not os.path.exists(file_path):\n            print(f\"Warning: File {file_path} not found on filesystem but exists in database\")\n            raise HTTPException(status_code=404, detail=f\"Workflow file '{filename}' not found on filesystem\")\n        \n        with open(file_path, 'r', encoding='utf-8') as f:\n            raw_json = json.load(f)\n        \n        return {\n            \"metadata\": workflow_meta,\n            \"raw_json\": raw_json\n        }\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error loading workflow: {str(e)}\")\n\n@app.get(\"/api/workflows/{filename}/download\")\nasync def download_workflow(filename: str):\n    \"\"\"Download workflow JSON file.\"\"\"\n    try:\n        file_path = os.path.join(\"workflows\", filename)\n        if not os.path.exists(file_path):\n            print(f\"Warning: Download requested for missing file: {file_path}\")\n            raise HTTPException(status_code=404, detail=f\"Workflow file '{filename}' not found on filesystem\")\n        \n        return FileResponse(\n            file_path,\n            media_type=\"application/json\",\n            filename=filename\n        )\n    except FileNotFoundError:\n        raise HTTPException(status_code=404, detail=f\"Workflow file '{filename}' not found\")\n    except Exception as e:\n        print(f\"Error downloading workflow {filename}: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Error downloading workflow: {str(e)}\")\n\n@app.get(\"/api/workflows/{filename}/diagram\")\nasync def get_workflow_diagram(filename: str):\n    \"\"\"Get Mermaid diagram code for workflow visualization.\"\"\"\n    try:\n        file_path = os.path.join(\"workflows\", filename)\n        if not os.path.exists(file_path):\n            print(f\"Warning: Diagram requested for missing file: {file_path}\")\n            raise HTTPException(status_code=404, detail=f\"Workflow file '{filename}' not found on filesystem\")\n        \n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        \n        nodes = data.get('nodes', [])\n        connections = data.get('connections', {})\n        \n        # Generate Mermaid diagram\n        diagram = generate_mermaid_diagram(nodes, connections)\n        \n        return {\"diagram\": diagram}\n    except HTTPException:\n        raise\n    except FileNotFoundError:\n        raise HTTPException(status_code=404, detail=f\"Workflow file '{filename}' not found\")\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing JSON in {filename}: {str(e)}\")\n        raise HTTPException(status_code=400, detail=f\"Invalid JSON in workflow file: {str(e)}\")\n    except Exception as e:\n        print(f\"Error generating diagram for {filename}: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Error generating diagram: {str(e)}\")\n\ndef generate_mermaid_diagram(nodes: List[Dict], connections: Dict) -> str:\n    \"\"\"Generate Mermaid.js flowchart code from workflow nodes and connections.\"\"\"\n    if not nodes:\n        return \"graph TD\\n  EmptyWorkflow[No nodes found in workflow]\"\n    \n    # Create mapping for node names to ensure valid mermaid IDs\n    mermaid_ids = {}\n    for i, node in enumerate(nodes):\n        node_id = f\"node{i}\"\n        node_name = node.get('name', f'Node {i}')\n        mermaid_ids[node_name] = node_id\n    \n    # Start building the mermaid diagram\n    mermaid_code = [\"graph TD\"]\n    \n    # Add nodes with styling\n    for node in nodes:\n        node_name = node.get('name', 'Unnamed')\n        node_id = mermaid_ids[node_name]\n        node_type = node.get('type', '').replace('dr.x-nodes-base.', '')\n        \n        # Determine node style based on type\n        style = \"\"\n        if any(x in node_type.lower() for x in ['trigger', 'webhook', 'cron']):\n            style = \"fill:#b3e0ff,stroke:#0066cc\"  # Blue for triggers\n        elif any(x in node_type.lower() for x in ['if', 'switch']):\n            style = \"fill:#ffffb3,stroke:#e6e600\"  # Yellow for conditional nodes\n        elif any(x in node_type.lower() for x in ['function', 'code']):\n            style = \"fill:#d9b3ff,stroke:#6600cc\"  # Purple for code nodes\n        elif 'error' in node_type.lower():\n            style = \"fill:#ffb3b3,stroke:#cc0000\"  # Red for error handlers\n        else:\n            style = \"fill:#d9d9d9,stroke:#666666\"  # Gray for other nodes\n        \n        # Add node with label (escaping special characters)\n        clean_name = node_name.replace('\"', \"'\")\n        clean_type = node_type.replace('\"', \"'\")\n        label = f\"{clean_name}<br>({clean_type})\"\n        mermaid_code.append(f\"  {node_id}[\\\"{label}\\\"]\")\n        mermaid_code.append(f\"  style {node_id} {style}\")\n    \n    # Add connections between nodes\n    for source_name, source_connections in connections.items():\n        if source_name not in mermaid_ids:\n            continue\n        \n        if isinstance(source_connections, dict) and 'main' in source_connections:\n            main_connections = source_connections['main']\n            \n            for i, output_connections in enumerate(main_connections):\n                if not isinstance(output_connections, list):\n                    continue\n                    \n                for connection in output_connections:\n                    if not isinstance(connection, dict) or 'node' not in connection:\n                        continue\n                        \n                    target_name = connection['node']\n                    if target_name not in mermaid_ids:\n                        continue\n                        \n                    # Add arrow with output index if multiple outputs\n                    label = f\" -->|{i}| \" if len(main_connections) > 1 else \" --> \"\n                    mermaid_code.append(f\"  {mermaid_ids[source_name]}{label}{mermaid_ids[target_name]}\")\n    \n    # Format the final mermaid diagram code\n    return \"\\n\".join(mermaid_code)\n\n@app.post(\"/api/reindex\")\nasync def reindex_workflows(background_tasks: BackgroundTasks, force: bool = False):\n    \"\"\"Trigger workflow reindexing in the background.\"\"\"\n    def run_indexing():\n        db.index_all_workflows(force_reindex=force)\n    \n    background_tasks.add_task(run_indexing)\n    return {\"message\": \"Reindexing started in background\"}\n\n@app.get(\"/api/integrations\")\nasync def get_integrations():\n    \"\"\"Get list of all unique integrations.\"\"\"\n    try:\n        stats = db.get_stats()\n        # For now, return basic info. Could be enhanced to return detailed integration stats\n        return {\"integrations\": [], \"count\": stats['unique_integrations']}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error fetching integrations: {str(e)}\")\n\n@app.get(\"/api/categories\")\nasync def get_categories():\n    \"\"\"Get available service categories for filtering.\"\"\"\n    try:\n        categories = db.get_service_categories()\n        return {\"categories\": categories}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error fetching categories: {str(e)}\")\n\n@app.get(\"/api/workflows/category/{category}\", response_model=SearchResponse)\nasync def search_workflows_by_category(\n    category: str,\n    page: int = Query(1, ge=1, description=\"Page number\"),\n    per_page: int = Query(20, ge=1, le=100, description=\"Items per page\")\n):\n    \"\"\"Search workflows by service category (messaging, database, ai_ml, etc.).\"\"\"\n    try:\n        offset = (page - 1) * per_page\n        \n        workflows, total = db.search_by_category(\n            category=category,\n            limit=per_page,\n            offset=offset\n        )\n        \n        # Convert to Pydantic models with error handling\n        workflow_summaries = []\n        for workflow in workflows:\n            try:\n                clean_workflow = {\n                    'id': workflow.get('id'),\n                    'filename': workflow.get('filename', ''),\n                    'name': workflow.get('name', ''),\n                    'active': workflow.get('active', False),\n                    'description': workflow.get('description', ''),\n                    'trigger_type': workflow.get('trigger_type', 'Manual'),\n                    'complexity': workflow.get('complexity', 'low'),\n                    'node_count': workflow.get('node_count', 0),\n                    'integrations': workflow.get('integrations', []),\n                    'tags': workflow.get('tags', []),\n                    'created_at': workflow.get('created_at'),\n                    'updated_at': workflow.get('updated_at')\n                }\n                workflow_summaries.append(WorkflowSummary(**clean_workflow))\n            except Exception as e:\n                print(f\"Error converting workflow {workflow.get('filename', 'unknown')}: {e}\")\n                continue\n        \n        pages = (total + per_page - 1) // per_page\n        \n        return SearchResponse(\n            workflows=workflow_summaries,\n            total=total,\n            page=page,\n            per_page=per_page,\n            pages=pages,\n            query=f\"category:{category}\",\n            filters={\"category\": category}\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error searching by category: {str(e)}\")\n\n# Custom exception handler for better error responses\n@app.exception_handler(Exception)\nasync def global_exception_handler(request, exc):\n    return JSONResponse(\n        status_code=500,\n        content={\"detail\": f\"Internal server error: {str(exc)}\"}\n    )\n\n# Mount static files AFTER all routes are defined\nstatic_dir = Path(\"static\")\nif static_dir.exists():\n    app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n    print(f\"\u2705 Static files mounted from {static_dir.absolute()}\")\nelse:\n    print(f\"\u274c Warning: Static directory not found at {static_dir.absolute()}\")\n\ndef create_static_directory():\n    \"\"\"Create static directory if it doesn't exist.\"\"\"\n    static_dir = Path(\"static\")\n    static_dir.mkdir(exist_ok=True)\n    return static_dir\n\ndef run_server(host: str = \"127.0.0.1\", port: int = 8000, reload: bool = False):\n    \"\"\"Run the FastAPI server.\"\"\"\n    # Ensure static directory exists\n    create_static_directory()\n    \n    # Debug: Check database connectivity\n    try:\n        stats = db.get_stats()\n        print(f\"\u2705 Database connected: {stats['total']} workflows found\")\n        if stats['total'] == 0:\n            print(\"\ud83d\udd04 Database is empty. Indexing workflows...\")\n            db.index_all_workflows()\n            stats = db.get_stats()\n    except Exception as e:\n        print(f\"\u274c Database error: {e}\")\n        print(\"\ud83d\udd04 Attempting to create and index database...\")\n        try:\n            db.index_all_workflows()\n            stats = db.get_stats()\n            print(f\"\u2705 Database created: {stats['total']} workflows indexed\")\n        except Exception as e2:\n            print(f\"\u274c Failed to create database: {e2}\")\n            stats = {'total': 0}\n    \n    # Debug: Check static files\n    static_path = Path(\"static\")\n    if static_path.exists():\n        files = list(static_path.glob(\"*\"))\n        print(f\"\u2705 Static files found: {[f.name for f in files]}\")\n    else:\n        print(f\"\u274c Static directory not found at: {static_path.absolute()}\")\n    \n    print(f\"\ud83d\ude80 Starting dr.x Workflow Documentation API\")\n    print(f\"\ud83d\udcca Database contains {stats['total']} workflows\")\n    print(f\"\ud83c\udf10 Server will be available at: http://{host}:{port}\")\n    print(f\"\ud83d\udcc1 Static files at: http://{host}:{port}/static/\")\n    \n    uvicorn.run(\n        \"api_server:app\",\n        host=host,\n        port=port,\n        reload=reload,\n        access_log=True,  # Enable access logs for debugging\n        log_level=\"info\"\n    )\n\nif __name__ == \"__main__\":\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='dr.x Workflow Documentation API Server')\n    parser.add_argument('--host', default='127.0.0.1', help='Host to bind to')\n    parser.add_argument('--port', type=int, default=8000, help='Port to bind to')\n    parser.add_argument('--reload', action='store_true', help='Enable auto-reload for development')\n    \n    args = parser.parse_args()\n    \n    run_server(host=args.host, port=args.port, reload=args.reload)\n",
      "size": 19146,
      "modified": "2025-08-30T07:20:27.888243"
    },
    "workflow_db.py": {
      "content": "#!/usr/bin/env python3\n\"\"\"\nFast dr.x Workflow Database\nSQLite-based workflow indexer and search engine for instant performance.\n\"\"\"\n\nimport sqlite3\nimport json\nimport os\nimport glob\nimport datetime\nimport hashlib\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom pathlib import Path\n\nclass WorkflowDatabase:\n    \"\"\"High-performance SQLite database for workflow metadata and search.\"\"\"\n    \n    def __init__(self, db_path: str = None):\n        # Use environment variable if no path provided\n        if db_path is None:\n            db_path = os.environ.get('WORKFLOW_DB_PATH', 'workflows.db')\n        self.db_path = db_path\n        self.workflows_dir = \"workflows\"\n        self.init_database()\n    \n    def init_database(self):\n        \"\"\"Initialize SQLite database with optimized schema and indexes.\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        conn.execute(\"PRAGMA journal_mode=WAL\")  # Write-ahead logging for performance\n        conn.execute(\"PRAGMA synchronous=NORMAL\")\n        conn.execute(\"PRAGMA cache_size=10000\")\n        conn.execute(\"PRAGMA temp_store=MEMORY\")\n        \n        # Create main workflows table\n        conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS workflows (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                filename TEXT UNIQUE NOT NULL,\n                name TEXT NOT NULL,\n                workflow_id TEXT,\n                active BOOLEAN DEFAULT 0,\n                description TEXT,\n                trigger_type TEXT,\n                complexity TEXT,\n                node_count INTEGER DEFAULT 0,\n                integrations TEXT,  -- JSON array\n                tags TEXT,         -- JSON array\n                created_at TEXT,\n                updated_at TEXT,\n                file_hash TEXT,\n                file_size INTEGER,\n                analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\")\n        \n        # Create FTS5 table for full-text search\n        conn.execute(\"\"\"\n            CREATE VIRTUAL TABLE IF NOT EXISTS workflows_fts USING fts5(\n                filename,\n                name,\n                description,\n                integrations,\n                tags,\n                content=workflows,\n                content_rowid=id\n            )\n        \"\"\")\n        \n        # Create indexes for fast filtering\n        conn.execute(\"CREATE INDEX IF NOT EXISTS idx_trigger_type ON workflows(trigger_type)\")\n        conn.execute(\"CREATE INDEX IF NOT EXISTS idx_complexity ON workflows(complexity)\")\n        conn.execute(\"CREATE INDEX IF NOT EXISTS idx_active ON workflows(active)\")\n        conn.execute(\"CREATE INDEX IF NOT EXISTS idx_node_count ON workflows(node_count)\")\n        conn.execute(\"CREATE INDEX IF NOT EXISTS idx_filename ON workflows(filename)\")\n        \n        # Create triggers to keep FTS table in sync\n        conn.execute(\"\"\"\n            CREATE TRIGGER IF NOT EXISTS workflows_ai AFTER INSERT ON workflows BEGIN\n                INSERT INTO workflows_fts(rowid, filename, name, description, integrations, tags)\n                VALUES (new.id, new.filename, new.name, new.description, new.integrations, new.tags);\n            END\n        \"\"\")\n        \n        conn.execute(\"\"\"\n            CREATE TRIGGER IF NOT EXISTS workflows_ad AFTER DELETE ON workflows BEGIN\n                INSERT INTO workflows_fts(workflows_fts, rowid, filename, name, description, integrations, tags)\n                VALUES ('delete', old.id, old.filename, old.name, old.description, old.integrations, old.tags);\n            END\n        \"\"\")\n        \n        conn.execute(\"\"\"\n            CREATE TRIGGER IF NOT EXISTS workflows_au AFTER UPDATE ON workflows BEGIN\n                INSERT INTO workflows_fts(workflows_fts, rowid, filename, name, description, integrations, tags)\n                VALUES ('delete', old.id, old.filename, old.name, old.description, old.integrations, old.tags);\n                INSERT INTO workflows_fts(rowid, filename, name, description, integrations, tags)\n                VALUES (new.id, new.filename, new.name, new.description, new.integrations, new.tags);\n            END\n        \"\"\")\n        \n        conn.commit()\n        conn.close()\n    \n    def get_file_hash(self, file_path: str) -> str:\n        \"\"\"Get MD5 hash of file for change detection.\"\"\"\n        hash_md5 = hashlib.md5()\n        with open(file_path, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n    \n    def format_workflow_name(self, filename: str) -> str:\n        \"\"\"Convert filename to readable workflow name.\"\"\"\n        # Remove .json extension\n        name = filename.replace('.json', '')\n        \n        # Split by underscores\n        parts = name.split('_')\n        \n        # Skip the first part if it's just a number\n        if len(parts) > 1 and parts[0].isdigit():\n            parts = parts[1:]\n        \n        # Convert parts to title case and join with spaces\n        readable_parts = []\n        for part in parts:\n            # Special handling for common terms\n            if part.lower() == 'http':\n                readable_parts.append('HTTP')\n            elif part.lower() == 'api':\n                readable_parts.append('API')\n            elif part.lower() == 'webhook':\n                readable_parts.append('Webhook')\n            elif part.lower() == 'automation':\n                readable_parts.append('Automation')\n            elif part.lower() == 'automate':\n                readable_parts.append('Automate')\n            elif part.lower() == 'scheduled':\n                readable_parts.append('Scheduled')\n            elif part.lower() == 'triggered':\n                readable_parts.append('Triggered')\n            elif part.lower() == 'manual':\n                readable_parts.append('Manual')\n            else:\n                # Capitalize first letter\n                readable_parts.append(part.capitalize())\n        \n        return ' '.join(readable_parts)\n    \n    def analyze_workflow_file(self, file_path: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Analyze a single workflow file and extract metadata.\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n        except (json.JSONDecodeError, UnicodeDecodeError) as e:\n            print(f\"Error reading {file_path}: {str(e)}\")\n            return None\n        \n        filename = os.path.basename(file_path)\n        file_size = os.path.getsize(file_path)\n        file_hash = self.get_file_hash(file_path)\n        \n        # Extract basic metadata\n        workflow = {\n            'filename': filename,\n            'name': self.format_workflow_name(filename),\n            'workflow_id': data.get('id', ''),\n            'active': data.get('active', False),\n            'nodes': data.get('nodes', []),\n            'connections': data.get('connections', {}),\n            'tags': data.get('tags', []),\n            'created_at': data.get('createdAt', ''),\n            'updated_at': data.get('updatedAt', ''),\n            'file_hash': file_hash,\n            'file_size': file_size\n        }\n        \n        # Use JSON name if available and meaningful, otherwise use formatted filename\n        json_name = data.get('name', '').strip()\n        if json_name and json_name != filename.replace('.json', '') and not json_name.startswith('My workflow'):\n            workflow['name'] = json_name\n        # If no meaningful JSON name, use formatted filename (already set above)\n        \n        # Analyze nodes\n        node_count = len(workflow['nodes'])\n        workflow['node_count'] = node_count\n        \n        # Determine complexity\n        if node_count <= 5:\n            complexity = 'low'\n        elif node_count <= 15:\n            complexity = 'medium'\n        else:\n            complexity = 'high'\n        workflow['complexity'] = complexity\n        \n        # Find trigger type and integrations\n        trigger_type, integrations = self.analyze_nodes(workflow['nodes'])\n        workflow['trigger_type'] = trigger_type\n        workflow['integrations'] = list(integrations)\n        \n        # Generate description\n        workflow['description'] = self.generate_description(workflow, trigger_type, integrations)\n        \n        return workflow\n    \n    def analyze_nodes(self, nodes: List[Dict]) -> Tuple[str, set]:\n        \"\"\"Analyze nodes to determine trigger type and integrations.\"\"\"\n        trigger_type = 'Manual'\n        integrations = set()\n        \n        # Enhanced service mapping for better recognition\n        service_mappings = {\n            # Messaging & Communication\n            'telegram': 'Telegram',\n            'telegramTrigger': 'Telegram',\n            'discord': 'Discord',\n            'slack': 'Slack', \n            'whatsapp': 'WhatsApp',\n            'mattermost': 'Mattermost',\n            'teams': 'Microsoft Teams',\n            'rocketchat': 'Rocket.Chat',\n            \n            # Email\n            'gmail': 'Gmail',\n            'mailjet': 'Mailjet',\n            'emailreadimap': 'Email (IMAP)',\n            'emailsendsmt': 'Email (SMTP)',\n            'outlook': 'Outlook',\n            \n            # Cloud Storage\n            'googledrive': 'Google Drive',\n            'googledocs': 'Google Docs',\n            'googlesheets': 'Google Sheets',\n            'dropbox': 'Dropbox',\n            'onedrive': 'OneDrive',\n            'box': 'Box',\n            \n            # Databases\n            'postgres': 'PostgreSQL',\n            'mysql': 'MySQL',\n            'mongodb': 'MongoDB',\n            'redis': 'Redis',\n            'airtable': 'Airtable',\n            'notion': 'Notion',\n            \n            # Project Management\n            'jira': 'Jira',\n            'github': 'GitHub',\n            'gitlab': 'GitLab',\n            'trello': 'Trello',\n            'asana': 'Asana',\n            'mondaycom': 'Monday.com',\n            \n            # AI/ML Services\n            'openai': 'OpenAI',\n            'anthropic': 'Anthropic',\n            'huggingface': 'Hugging Face',\n            \n            # Social Media\n            'linkedin': 'LinkedIn',\n            'twitter': 'Twitter/X',\n            'facebook': 'Facebook',\n            'instagram': 'Instagram',\n            \n            # E-commerce\n            'shopify': 'Shopify',\n            'stripe': 'Stripe',\n            'paypal': 'PayPal',\n            \n            # Analytics\n            'googleanalytics': 'Google Analytics',\n            'mixpanel': 'Mixpanel',\n            \n            # Calendar & Tasks\n            'googlecalendar': 'Google Calendar', \n            'googletasks': 'Google Tasks',\n            'cal': 'Cal.com',\n            'calendly': 'Calendly',\n            \n            # Forms & Surveys\n            'typeform': 'Typeform',\n            'googleforms': 'Google Forms',\n            'form': 'Form Trigger',\n            \n            # Development Tools\n            'webhook': 'Webhook',\n            'httpRequest': 'HTTP Request',\n            'graphql': 'GraphQL',\n            'sse': 'Server-Sent Events',\n            \n            # Utility nodes (exclude from integrations)\n            'set': None,\n            'function': None,\n            'code': None,\n            'if': None,\n            'switch': None,\n            'merge': None,\n            'split': None,\n            'stickynote': None,\n            'stickyNote': None,\n            'wait': None,\n            'schedule': None,\n            'cron': None,\n            'manual': None,\n            'stopanderror': None,\n            'noop': None,\n            'noOp': None,\n            'error': None,\n            'limit': None,\n            'aggregate': None,\n            'summarize': None,\n            'filter': None,\n            'sort': None,\n            'removeDuplicates': None,\n            'dateTime': None,\n            'extractFromFile': None,\n            'convertToFile': None,\n            'readBinaryFile': None,\n            'readBinaryFiles': None,\n            'executionData': None,\n            'executeWorkflow': None,\n            'executeCommand': None,\n            'respondToWebhook': None,\n        }\n        \n        for node in nodes:\n            node_type = node.get('type', '')\n            node_name = node.get('name', '').lower()\n            \n            # Determine trigger type\n            if 'webhook' in node_type.lower() or 'webhook' in node_name:\n                trigger_type = 'Webhook'\n            elif 'cron' in node_type.lower() or 'schedule' in node_type.lower():\n                trigger_type = 'Scheduled'\n            elif 'trigger' in node_type.lower() and trigger_type == 'Manual':\n                if 'manual' not in node_type.lower():\n                    trigger_type = 'Webhook'\n            \n            # Extract integrations with enhanced mapping\n            service_name = None\n            \n            # Handle dr.x-nodes-base nodes\n            if node_type.startswith('dr.x-nodes-base.'):\n                raw_service = node_type.replace('dr.x-nodes-base.', '').lower()\n                raw_service = raw_service.replace('trigger', '')\n                service_name = service_mappings.get(raw_service, raw_service.title() if raw_service else None)\n            \n            # Handle @dr.x/ namespaced nodes\n            elif node_type.startswith('@dr.x/'):\n                raw_service = node_type.split('.')[-1].lower() if '.' in node_type else node_type.lower()\n                raw_service = raw_service.replace('trigger', '')\n                service_name = service_mappings.get(raw_service, raw_service.title() if raw_service else None)\n            \n            # Handle custom nodes\n            elif '-' in node_type:\n                # Try to extract service name from custom node names like \"dr.x-nodes-youtube-transcription-kasha.youtubeTranscripter\"\n                parts = node_type.lower().split('.')\n                for part in parts:\n                    if 'youtube' in part:\n                        service_name = 'YouTube'\n                        break\n                    elif 'telegram' in part:\n                        service_name = 'Telegram'\n                        break\n                    elif 'discord' in part:\n                        service_name = 'Discord'\n                        break\n            \n            # Also check node names for service hints\n            for service_key, service_value in service_mappings.items():\n                if service_key in node_name and service_value:\n                    service_name = service_value\n                    break\n            \n            # Add to integrations if valid service found\n            if service_name and service_name not in ['None', None]:\n                integrations.add(service_name)\n        \n        # Determine if complex based on node variety and count\n        if len(nodes) > 10 and len(integrations) > 3:\n            trigger_type = 'Complex'\n        \n        return trigger_type, integrations\n    \n    def generate_description(self, workflow: Dict, trigger_type: str, integrations: set) -> str:\n        \"\"\"Generate a descriptive summary of the workflow.\"\"\"\n        name = workflow['name']\n        node_count = workflow['node_count']\n        \n        # Start with trigger description\n        trigger_descriptions = {\n            'Webhook': \"Webhook-triggered automation that\",\n            'Scheduled': \"Scheduled automation that\", \n            'Complex': \"Complex multi-step automation that\",\n        }\n        desc = trigger_descriptions.get(trigger_type, \"Manual workflow that\")\n        \n        # Add functionality based on name and integrations\n        if integrations:\n            main_services = list(integrations)[:3]\n            if len(main_services) == 1:\n                desc += f\" integrates with {main_services[0]}\"\n            elif len(main_services) == 2:\n                desc += f\" connects {main_services[0]} and {main_services[1]}\"\n            else:\n                desc += f\" orchestrates {', '.join(main_services[:-1])}, and {main_services[-1]}\"\n        \n        # Add workflow purpose hints from name\n        name_lower = name.lower()\n        if 'create' in name_lower:\n            desc += \" to create new records\"\n        elif 'update' in name_lower:\n            desc += \" to update existing data\"\n        elif 'sync' in name_lower:\n            desc += \" to synchronize data\"\n        elif 'notification' in name_lower or 'alert' in name_lower:\n            desc += \" for notifications and alerts\"\n        elif 'backup' in name_lower:\n            desc += \" for data backup operations\"\n        elif 'monitor' in name_lower:\n            desc += \" for monitoring and reporting\"\n        else:\n            desc += \" for data processing\"\n        \n        desc += f\". Uses {node_count} nodes\"\n        if len(integrations) > 3:\n            desc += f\" and integrates with {len(integrations)} services\"\n        \n        return desc + \".\"\n    \n    def index_all_workflows(self, force_reindex: bool = False) -> Dict[str, int]:\n        \"\"\"Index all workflow files. Only reprocesses changed files unless force_reindex=True.\"\"\"\n        if not os.path.exists(self.workflows_dir):\n            print(f\"Warning: Workflows directory '{self.workflows_dir}' not found.\")\n            return {'processed': 0, 'skipped': 0, 'errors': 0}\n        \n        json_files = glob.glob(os.path.join(self.workflows_dir, \"*.json\"))\n        \n        if not json_files:\n            print(f\"Warning: No JSON files found in '{self.workflows_dir}' directory.\")\n            return {'processed': 0, 'skipped': 0, 'errors': 0}\n        \n        print(f\"Indexing {len(json_files)} workflow files...\")\n        \n        conn = sqlite3.connect(self.db_path)\n        conn.row_factory = sqlite3.Row\n        \n        stats = {'processed': 0, 'skipped': 0, 'errors': 0}\n        \n        for file_path in json_files:\n            filename = os.path.basename(file_path)\n            \n            try:\n                # Check if file needs to be reprocessed\n                if not force_reindex:\n                    current_hash = self.get_file_hash(file_path)\n                    cursor = conn.execute(\n                        \"SELECT file_hash FROM workflows WHERE filename = ?\", \n                        (filename,)\n                    )\n                    row = cursor.fetchone()\n                    if row and row['file_hash'] == current_hash:\n                        stats['skipped'] += 1\n                        continue\n                \n                # Analyze workflow\n                workflow_data = self.analyze_workflow_file(file_path)\n                if not workflow_data:\n                    stats['errors'] += 1\n                    continue\n                \n                # Insert or update in database\n                conn.execute(\"\"\"\n                    INSERT OR REPLACE INTO workflows (\n                        filename, name, workflow_id, active, description, trigger_type,\n                        complexity, node_count, integrations, tags, created_at, updated_at,\n                        file_hash, file_size, analyzed_at\n                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)\n                \"\"\", (\n                    workflow_data['filename'],\n                    workflow_data['name'],\n                    workflow_data['workflow_id'],\n                    workflow_data['active'],\n                    workflow_data['description'],\n                    workflow_data['trigger_type'],\n                    workflow_data['complexity'],\n                    workflow_data['node_count'],\n                    json.dumps(workflow_data['integrations']),\n                    json.dumps(workflow_data['tags']),\n                    workflow_data['created_at'],\n                    workflow_data['updated_at'],\n                    workflow_data['file_hash'],\n                    workflow_data['file_size']\n                ))\n                \n                stats['processed'] += 1\n                \n            except Exception as e:\n                print(f\"Error processing {file_path}: {str(e)}\")\n                stats['errors'] += 1\n                continue\n        \n        conn.commit()\n        conn.close()\n        \n        print(f\"\u2705 Indexing complete: {stats['processed']} processed, {stats['skipped']} skipped, {stats['errors']} errors\")\n        return stats\n    \n    def search_workflows(self, query: str = \"\", trigger_filter: str = \"all\", \n                        complexity_filter: str = \"all\", active_only: bool = False,\n                        limit: int = 50, offset: int = 0) -> Tuple[List[Dict], int]:\n        \"\"\"Fast search with filters and pagination.\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        conn.row_factory = sqlite3.Row\n        \n        # Build WHERE clause\n        where_conditions = []\n        params = []\n        \n        if active_only:\n            where_conditions.append(\"w.active = 1\")\n        \n        if trigger_filter != \"all\":\n            where_conditions.append(\"w.trigger_type = ?\")\n            params.append(trigger_filter)\n        \n        if complexity_filter != \"all\":\n            where_conditions.append(\"w.complexity = ?\")\n            params.append(complexity_filter)\n        \n        # Use FTS search if query provided\n        if query.strip():\n            # FTS search with ranking\n            base_query = \"\"\"\n                SELECT w.*, rank\n                FROM workflows_fts fts\n                JOIN workflows w ON w.id = fts.rowid\n                WHERE workflows_fts MATCH ?\n            \"\"\"\n            params.insert(0, query)\n        else:\n            # Regular query without FTS\n            base_query = \"\"\"\n                SELECT w.*, 0 as rank\n                FROM workflows w\n                WHERE 1=1\n            \"\"\"\n        \n        if where_conditions:\n            base_query += \" AND \" + \" AND \".join(where_conditions)\n        \n        # Count total results\n        count_query = f\"SELECT COUNT(*) as total FROM ({base_query}) t\"\n        cursor = conn.execute(count_query, params)\n        total = cursor.fetchone()['total']\n        \n        # Get paginated results\n        if query.strip():\n            base_query += \" ORDER BY rank\"\n        else:\n            base_query += \" ORDER BY w.analyzed_at DESC\"\n        \n        base_query += f\" LIMIT {limit} OFFSET {offset}\"\n        \n        cursor = conn.execute(base_query, params)\n        rows = cursor.fetchall()\n        \n        # Convert to dictionaries and parse JSON fields\n        results = []\n        for row in rows:\n            workflow = dict(row)\n            workflow['integrations'] = json.loads(workflow['integrations'] or '[]')\n            \n            # Parse tags and convert dict tags to strings\n            raw_tags = json.loads(workflow['tags'] or '[]')\n            clean_tags = []\n            for tag in raw_tags:\n                if isinstance(tag, dict):\n                    # Extract name from tag dict if available\n                    clean_tags.append(tag.get('name', str(tag.get('id', 'tag'))))\n                else:\n                    clean_tags.append(str(tag))\n            workflow['tags'] = clean_tags\n            \n            results.append(workflow)\n        \n        conn.close()\n        return results, total\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get database statistics.\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        conn.row_factory = sqlite3.Row\n        \n        # Basic counts\n        cursor = conn.execute(\"SELECT COUNT(*) as total FROM workflows\")\n        total = cursor.fetchone()['total']\n        \n        cursor = conn.execute(\"SELECT COUNT(*) as active FROM workflows WHERE active = 1\")\n        active = cursor.fetchone()['active']\n        \n        # Trigger type breakdown\n        cursor = conn.execute(\"\"\"\n            SELECT trigger_type, COUNT(*) as count \n            FROM workflows \n            GROUP BY trigger_type\n        \"\"\")\n        triggers = {row['trigger_type']: row['count'] for row in cursor.fetchall()}\n        \n        # Complexity breakdown\n        cursor = conn.execute(\"\"\"\n            SELECT complexity, COUNT(*) as count \n            FROM workflows \n            GROUP BY complexity\n        \"\"\")\n        complexity = {row['complexity']: row['count'] for row in cursor.fetchall()}\n        \n        # Node stats\n        cursor = conn.execute(\"SELECT SUM(node_count) as total_nodes FROM workflows\")\n        total_nodes = cursor.fetchone()['total_nodes'] or 0\n        \n        # Unique integrations count\n        cursor = conn.execute(\"SELECT integrations FROM workflows WHERE integrations != '[]'\")\n        all_integrations = set()\n        for row in cursor.fetchall():\n            integrations = json.loads(row['integrations'])\n            all_integrations.update(integrations)\n        \n        conn.close()\n        \n        return {\n            'total': total,\n            'active': active,\n            'inactive': total - active,\n            'triggers': triggers,\n            'complexity': complexity,\n            'total_nodes': total_nodes,\n            'unique_integrations': len(all_integrations),\n            'last_indexed': datetime.datetime.now().isoformat()\n        }\n\n    def get_service_categories(self) -> Dict[str, List[str]]:\n        \"\"\"Get service categories for enhanced filtering.\"\"\"\n        return {\n            'messaging': ['Telegram', 'Discord', 'Slack', 'WhatsApp', 'Mattermost', 'Microsoft Teams', 'Rocket.Chat'],\n            'email': ['Gmail', 'Mailjet', 'Email (IMAP)', 'Email (SMTP)', 'Outlook'],\n            'cloud_storage': ['Google Drive', 'Google Docs', 'Google Sheets', 'Dropbox', 'OneDrive', 'Box'],\n            'database': ['PostgreSQL', 'MySQL', 'MongoDB', 'Redis', 'Airtable', 'Notion'],\n            'project_management': ['Jira', 'GitHub', 'GitLab', 'Trello', 'Asana', 'Monday.com'],\n            'ai_ml': ['OpenAI', 'Anthropic', 'Hugging Face'],\n            'social_media': ['LinkedIn', 'Twitter/X', 'Facebook', 'Instagram'],\n            'ecommerce': ['Shopify', 'Stripe', 'PayPal'],\n            'analytics': ['Google Analytics', 'Mixpanel'],\n            'calendar_tasks': ['Google Calendar', 'Google Tasks', 'Cal.com', 'Calendly'],\n            'forms': ['Typeform', 'Google Forms', 'Form Trigger'],\n            'development': ['Webhook', 'HTTP Request', 'GraphQL', 'Server-Sent Events', 'YouTube']\n        }\n\n    def search_by_category(self, category: str, limit: int = 50, offset: int = 0) -> Tuple[List[Dict], int]:\n        \"\"\"Search workflows by service category.\"\"\"\n        categories = self.get_service_categories()\n        if category not in categories:\n            return [], 0\n        \n        services = categories[category]\n        conn = sqlite3.connect(self.db_path)\n        conn.row_factory = sqlite3.Row\n        \n        # Build OR conditions for all services in category\n        service_conditions = []\n        params = []\n        for service in services:\n            service_conditions.append(\"integrations LIKE ?\")\n            params.append(f'%\"{service}\"%')\n        \n        where_clause = \" OR \".join(service_conditions)\n        \n        # Count total results\n        count_query = f\"SELECT COUNT(*) as total FROM workflows WHERE {where_clause}\"\n        cursor = conn.execute(count_query, params)\n        total = cursor.fetchone()['total']\n        \n        # Get paginated results\n        query = f\"\"\"\n            SELECT * FROM workflows \n            WHERE {where_clause}\n            ORDER BY analyzed_at DESC\n            LIMIT {limit} OFFSET {offset}\n        \"\"\"\n        \n        cursor = conn.execute(query, params)\n        rows = cursor.fetchall()\n        \n        # Convert to dictionaries and parse JSON fields\n        results = []\n        for row in rows:\n            workflow = dict(row)\n            workflow['integrations'] = json.loads(workflow['integrations'] or '[]')\n            raw_tags = json.loads(workflow['tags'] or '[]')\n            clean_tags = []\n            for tag in raw_tags:\n                if isinstance(tag, dict):\n                    clean_tags.append(tag.get('name', str(tag.get('id', 'tag'))))\n                else:\n                    clean_tags.append(str(tag))\n            workflow['tags'] = clean_tags\n            results.append(workflow)\n        \n        conn.close()\n        return results, total\n\n\ndef main():\n    \"\"\"Command-line interface for workflow database.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='dr.x Workflow Database')\n    parser.add_argument('--index', action='store_true', help='Index all workflows')\n    parser.add_argument('--force', action='store_true', help='Force reindex all files')\n    parser.add_argument('--search', help='Search workflows')\n    parser.add_argument('--stats', action='store_true', help='Show database statistics')\n    \n    args = parser.parse_args()\n    \n    db = WorkflowDatabase()\n    \n    if args.index:\n        stats = db.index_all_workflows(force_reindex=args.force)\n        print(f\"Indexed {stats['processed']} workflows\")\n    \n    elif args.search:\n        results, total = db.search_workflows(args.search, limit=10)\n        print(f\"Found {total} workflows:\")\n        for workflow in results:\n            print(f\"  - {workflow['name']} ({workflow['trigger_type']}, {workflow['node_count']} nodes)\")\n    \n    elif args.stats:\n        stats = db.get_stats()\n        print(f\"Database Statistics:\")\n        print(f\"  Total workflows: {stats['total']}\")\n        print(f\"  Active: {stats['active']}\")\n        print(f\"  Total nodes: {stats['total_nodes']}\")\n        print(f\"  Unique integrations: {stats['unique_integrations']}\")\n        print(f\"  Trigger types: {stats['triggers']}\")\n    \n    else:\n        parser.print_help()\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "size": 29680,
      "modified": "2025-08-30T07:24:25.323578"
    },
    "run.py": {
      "content": "#!/usr/bin/env python3\n\"\"\"\n\ud83d\ude80 dr.x Workflows Search Engine Launcher\nStart the advanced search system with optimized performance.\n\"\"\"\n\nimport sys\nimport os\nimport argparse\nfrom pathlib import Path\n\n\ndef print_banner():\n    \"\"\"Print application banner.\"\"\"\n    print(\"\ud83d\ude80 dr.x-workflows Advanced Search Engine\")\n    print(\"=\" * 50)\n\n\ndef check_requirements() -> bool:\n    \"\"\"Check if required dependencies are installed.\"\"\"\n    missing_deps = []\n    \n    try:\n        import sqlite3\n    except ImportError:\n        missing_deps.append(\"sqlite3\")\n    \n    try:\n        import uvicorn\n    except ImportError:\n        missing_deps.append(\"uvicorn\")\n    \n    try:\n        import fastapi\n    except ImportError:\n        missing_deps.append(\"fastapi\")\n    \n    if missing_deps:\n        print(f\"\u274c Missing dependencies: {', '.join(missing_deps)}\")\n        print(\"\ud83d\udca1 Install with: pip install -r requirements.txt\")\n        return False\n    \n    print(\"\u2705 Dependencies verified\")\n    return True\n\n\ndef setup_directories():\n    \"\"\"Create necessary directories.\"\"\"\n    directories = [\"database\", \"static\", \"workflows\"]\n    \n    for directory in directories:\n        os.makedirs(directory, exist_ok=True)\n    \n    print(\"\u2705 Directories verified\")\n\n\ndef setup_database(force_reindex: bool = False) -> str:\n    \"\"\"Setup and initialize the database.\"\"\"\n    from workflow_db import WorkflowDatabase\n    \n    db_path = \"database/workflows.db\"\n    \n    print(f\"\ud83d\udd04 Setting up database: {db_path}\")\n    db = WorkflowDatabase(db_path)\n    \n    # Check if database has data or force reindex\n    stats = db.get_stats()\n    if stats['total'] == 0 or force_reindex:\n        print(\"\ud83d\udcda Indexing workflows...\")\n        index_stats = db.index_all_workflows(force_reindex=True)\n        print(f\"\u2705 Indexed {index_stats['processed']} workflows\")\n        \n        # Show final stats\n        final_stats = db.get_stats()\n        print(f\"\ud83d\udcca Database contains {final_stats['total']} workflows\")\n    else:\n        print(f\"\u2705 Database ready: {stats['total']} workflows\")\n    \n    return db_path\n\n\ndef start_server(host: str = \"127.0.0.1\", port: int = 8000, reload: bool = False):\n    \"\"\"Start the FastAPI server.\"\"\"\n    print(f\"\ud83c\udf10 Starting server at http://{host}:{port}\")\n    print(f\"\ud83d\udcca API Documentation: http://{host}:{port}/docs\")\n    print(f\"\ud83d\udd0d Workflow Search: http://{host}:{port}/api/workflows\")\n    print()\n    print(\"Press Ctrl+C to stop the server\")\n    print(\"-\" * 50)\n    \n    # Configure database path\n    os.environ['WORKFLOW_DB_PATH'] = \"database/workflows.db\"\n    \n    # Start uvicorn with better configuration\n    import uvicorn\n    uvicorn.run(\n        \"api_server:app\", \n        host=host, \n        port=port, \n        reload=reload,\n        log_level=\"info\",\n        access_log=False  # Reduce log noise\n    )\n\n\ndef main():\n    \"\"\"Main entry point with command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"dr.x Workflows Search Engine\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  python run.py                    # Start with default settings\n  python run.py --port 3000        # Start on port 3000\n  python run.py --host 0.0.0.0     # Accept external connections\n  python run.py --reindex          # Force database reindexing\n  python run.py --dev              # Development mode with auto-reload\n        \"\"\"\n    )\n    \n    parser.add_argument(\n        \"--host\", \n        default=\"127.0.0.1\", \n        help=\"Host to bind to (default: 127.0.0.1)\"\n    )\n    parser.add_argument(\n        \"--port\", \n        type=int, \n        default=8000, \n        help=\"Port to bind to (default: 8000)\"\n    )\n    parser.add_argument(\n        \"--reindex\", \n        action=\"store_true\", \n        help=\"Force database reindexing\"\n    )\n    parser.add_argument(\n        \"--dev\", \n        action=\"store_true\", \n        help=\"Development mode with auto-reload\"\n    )\n    \n    args = parser.parse_args()\n    \n    print_banner()\n    \n    # Check dependencies\n    if not check_requirements():\n        sys.exit(1)\n    \n    # Setup directories\n    setup_directories()\n    \n    # Setup database\n    try:\n        setup_database(force_reindex=args.reindex)\n    except Exception as e:\n        print(f\"\u274c Database setup error: {e}\")\n        sys.exit(1)\n    \n    # Start server\n    try:\n        start_server(\n            host=args.host, \n            port=args.port, \n            reload=args.dev\n        )\n    except KeyboardInterrupt:\n        print(\"\\n\ud83d\udc4b Server stopped!\")\n    except Exception as e:\n        print(f\"\u274c Server error: {e}\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main() \n",
      "size": 4655,
      "modified": "2025-08-30T07:25:36.930995"
    },
    "import_workflows.py": {
      "content": "#!/usr/bin/env python3\n\"\"\"\ndr.x Workflow Importer\nPython replacement for import-workflows.sh with better error handling and progress tracking.\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\n\nclass WorkflowImporter:\n    \"\"\"Import dr.x workflows with progress tracking and error handling.\"\"\"\n    \n    def __init__(self, workflows_dir: str = \"workflows\"):\n        self.workflows_dir = Path(workflows_dir)\n        self.imported_count = 0\n        self.failed_count = 0\n        self.errors = []\n\n    def validate_workflow(self, file_path: Path) -> bool:\n        \"\"\"Validate workflow JSON before import.\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            \n            # Basic validation\n            if not isinstance(data, dict):\n                return False\n            \n            # Check required fields\n            required_fields = ['nodes', 'connections']\n            for field in required_fields:\n                if field not in data:\n                    return False\n            \n            return True\n        except (json.JSONDecodeError, FileNotFoundError, PermissionError):\n            return False\n\n    def import_workflow(self, file_path: Path) -> bool:\n        \"\"\"Import a single workflow file.\"\"\"\n        try:\n            # Validate first\n            if not self.validate_workflow(file_path):\n                self.errors.append(f\"Invalid JSON: {file_path.name}\")\n                return False\n            \n            # Run dr.x import command\n            result = subprocess.run([\n                'npx', 'dr.x', 'import:workflow', \n                f'--input={file_path}'\n            ], capture_output=True, text=True, timeout=30)\n            \n            if result.returncode == 0:\n                print(f\"\u2705 Imported: {file_path.name}\")\n                return True\n            else:\n                error_msg = result.stderr.strip() or result.stdout.strip()\n                self.errors.append(f\"Import failed for {file_path.name}: {error_msg}\")\n                print(f\"\u274c Failed: {file_path.name}\")\n                return False\n                \n        except subprocess.TimeoutExpired:\n            self.errors.append(f\"Timeout importing {file_path.name}\")\n            print(f\"\u23f0 Timeout: {file_path.name}\")\n            return False\n        except Exception as e:\n            self.errors.append(f\"Error importing {file_path.name}: {str(e)}\")\n            print(f\"\u274c Error: {file_path.name} - {str(e)}\")\n            return False\n\n    def get_workflow_files(self) -> List[Path]:\n        \"\"\"Get all workflow JSON files.\"\"\"\n        if not self.workflows_dir.exists():\n            print(f\"\u274c Workflows directory not found: {self.workflows_dir}\")\n            return []\n        \n        json_files = list(self.workflows_dir.glob(\"*.json\"))\n        if not json_files:\n            print(f\"\u274c No JSON files found in: {self.workflows_dir}\")\n            return []\n        \n        return sorted(json_files)\n\n    def import_all(self) -> Dict[str, Any]:\n        \"\"\"Import all workflow files.\"\"\"\n        workflow_files = self.get_workflow_files()\n        total_files = len(workflow_files)\n        \n        if total_files == 0:\n            return {\"success\": False, \"message\": \"No workflow files found\"}\n        \n        print(f\"\ud83d\ude80 Starting import of {total_files} workflows...\")\n        print(\"-\" * 50)\n        \n        for i, file_path in enumerate(workflow_files, 1):\n            print(f\"[{i}/{total_files}] Processing {file_path.name}...\")\n            \n            if self.import_workflow(file_path):\n                self.imported_count += 1\n            else:\n                self.failed_count += 1\n        \n        # Summary\n        print(\"\\n\" + \"=\" * 50)\n        print(f\"\ud83d\udcca Import Summary:\")\n        print(f\"\u2705 Successfully imported: {self.imported_count}\")\n        print(f\"\u274c Failed imports: {self.failed_count}\")\n        print(f\"\ud83d\udcc1 Total files: {total_files}\")\n        \n        if self.errors:\n            print(f\"\\n\u274c Errors encountered:\")\n            for error in self.errors[:10]:  # Show first 10 errors\n                print(f\"   \u2022 {error}\")\n            if len(self.errors) > 10:\n                print(f\"   ... and {len(self.errors) - 10} more errors\")\n        \n        return {\n            \"success\": self.failed_count == 0,\n            \"imported\": self.imported_count,\n            \"failed\": self.failed_count,\n            \"total\": total_files,\n            \"errors\": self.errors\n        }\n\n\ndef check_dr.x_available() -> bool:\n    \"\"\"Check if dr.x CLI is available.\"\"\"\n    try:\n        result = subprocess.run(\n            ['npx', 'dr.x', '--version'], \n            capture_output=True, text=True, timeout=10\n        )\n        return result.returncode == 0\n    except (subprocess.TimeoutExpired, FileNotFoundError):\n        return False\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    print(\"\ud83d\udd27 dr.x Workflow Importer\")\n    print(\"=\" * 40)\n    \n    # Check if dr.x is available\n    if not check_dr.x_available():\n        print(\"\u274c dr.x CLI not found. Please install dr.x first:\")\n        print(\"   npm install -g dr.x\")\n        sys.exit(1)\n    \n    # Create importer and run\n    importer = WorkflowImporter()\n    result = importer.import_all()\n    \n    # Exit with appropriate code\n    sys.exit(0 if result[\"success\"] else 1)\n\n\nif __name__ == \"__main__\":\n    main() \n",
      "size": 5429,
      "modified": "2025-08-30T07:33:01.872549"
    }
  }
}